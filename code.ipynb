{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi2Jz96CIkf6"
      },
      "source": [
        "#Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dCYyKRWmEBO"
      },
      "outputs": [],
      "source": [
        "# JALANIN\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "from google.cloud import storage\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "import random\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mounting DATASET\n",
        "This step is optional, you can download the dataset from our github"
      ],
      "metadata": {
        "id": "x7FXtkjoODaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGrtgzk3twej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "923f1aa5-3eea-4d58-dfb6-fb426e10ca6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from google.colab import auth\\nauth.authenticate_user()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# OPTIONAL \n",
        "'''from google.colab import auth\n",
        "auth.authenticate_user()'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting BANGKIT GDrive\n",
        "# OPTIONAL\n",
        "# Change the PATH with ur own DATASET location\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bULl8rF0qW2",
        "outputId": "6980d577-19cd-4c5a-af2a-d5804ce0a652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z92qaLKTKsUR"
      },
      "outputs": [],
      "source": [
        "# Mounting GCP Cloud Bucket\n",
        "# Change the PATH with ur own DATASET location\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/drive/MyDrive/bdcc-colab.json' \n",
        "storage_client = storage.Client()\n",
        "bucket_name = 'c22ps191b2'\n",
        "print(bucket_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zy8rD5hrxBh-",
        "outputId": "794eaaf5-a46d-4f85-d117-4e118ec2f83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activated service account credentials for: [bdcc-colab@zeta-buckeye-351509.iam.gserviceaccount.com]\n"
          ]
        }
      ],
      "source": [
        "project='zeta-buckeye-351509' # change to your project name here\n",
        "os.environ['GCP_PROJECT'] = project \n",
        "os.environ['GCP_ACCOUNT'] = 'bdcc-colab@' + project + '.iam.gserviceaccount.com'\n",
        "\n",
        "!gcloud auth activate-service-account \"$GCP_ACCOUNT\" --key-file=\"$GOOGLE_APPLICATION_CREDENTIALS\" --project=\"$GCP_PROJECT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va9G39_NSPwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8518f37b-27f5-42fa-e641-05fdb6fb672d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BucketNotFoundException: 404 gs://{bucket_name} bucket does not exist.\n"
          ]
        }
      ],
      "source": [
        "# Saving Dataset to Cloud Bucket\n",
        "# GAUSAH DIJALANIN LAGI\n",
        "'!gsutil -m cp -r /content/drive/MyDrive/Datasets/* gs://{bucket_name}/dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST DISPLAY DATASET"
      ],
      "metadata": {
        "id": "IBPE5u69OJAa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MirwdT2smEBQ"
      },
      "outputs": [],
      "source": [
        "#Test Display\n",
        "# Change the PATH with ur own DATASET location\n",
        "from IPython.display import Image\n",
        "Image(\"/content/drive/MyDrive/Datasets/Train/Acne and Rosacea Photos/07Acne081101.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CKNQZLFa0d8"
      },
      "outputs": [],
      "source": [
        "# Test Display (2)\n",
        "# Change the PATH with ur own DATASET location\n",
        "plt.figure(figsize=(20,20))\n",
        "test_folder=r'/content/drive/MyDrive/Datasets/Test/Acne and Rosacea Photos'\n",
        "for i in range(5):\n",
        "    file = random.choice(os.listdir(test_folder))\n",
        "    image_path= os.path.join(test_folder, file)\n",
        "    img=mpimg.imread(image_path)\n",
        "    ax=plt.subplot(1,5,i+1)\n",
        "    ax.title.set_text(file)\n",
        "    plt.imshow(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpF3RMNJkoYy",
        "outputId": "aa3a3f37-5b43-4b11-cf03-a30a4e94c499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of base directory:\n",
            "['Test', 'Train', 'class_list.txt']\n",
            "\n",
            "Contents of train directory:\n",
            "['Poison Ivy Photos and other Contact Dermatitis', 'Herpes HPV and other STDs Photos', 'Acne and Rosacea Photos', 'Atopic Dermatitis Photos', 'Scabies Lyme Disease and other Infestations and Bites', 'Psoriasis pictures Lichen Planus and related diseases']\n",
            "\n",
            "Contents of validation directory:\n",
            "['Poison Ivy Photos and other Contact Dermatitis', 'Psoriasis pictures Lichen Planus and related diseases', 'Acne and Rosacea Photos', 'Atopic Dermatitis Photos', 'Scabies Lyme Disease and other Infestations and Bites', 'Herpes HPV and other STDs Photos']\n"
          ]
        }
      ],
      "source": [
        "# Change the PATH with ur own DATASET location\n",
        "base_dir = '/content/drive/MyDrive/Datasets'\n",
        "\n",
        "print(\"Contents of base directory:\")\n",
        "print(os.listdir(base_dir))\n",
        "\n",
        "print(\"\\nContents of train directory:\")\n",
        "print(os.listdir(f'{base_dir}/Train'))\n",
        "\n",
        "print(\"\\nContents of validation directory:\")\n",
        "print(os.listdir(f'{base_dir}/Test'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "-e07LyrVOOWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the PATH with ur own DATASET location\n",
        "train_path=\"/content/drive/MyDrive/Datasets/Train\"\n",
        "test_path=\"/content/drive/MyDrive/Datasets/Test\""
      ],
      "metadata": {
        "id": "rQ8KwFVx4axE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1tOjVR9uK8r"
      },
      "outputs": [],
      "source": [
        "x_train=[]\n",
        "\n",
        "for folder in os.listdir(train_path):\n",
        "\n",
        "    sub_path=train_path+\"/\"+folder\n",
        "\n",
        "    for img in os.listdir(sub_path):\n",
        "\n",
        "        image_path=sub_path+\"/\"+img\n",
        "\n",
        "        img_arr=cv2.imread(image_path)\n",
        "\n",
        "        img_arr=cv2.resize(img_arr,(150,150))\n",
        "\n",
        "        x_train.append(img_arr)\n",
        "\n",
        "x_test=[]\n",
        "\n",
        "for folder in os.listdir(test_path):\n",
        "\n",
        "    sub_path=test_path+\"/\"+folder\n",
        "\n",
        "    for img in os.listdir(sub_path):\n",
        "\n",
        "        image_path=sub_path+\"/\"+img\n",
        "\n",
        "        img_arr=cv2.imread(image_path)\n",
        "\n",
        "        img_arr=cv2.resize(img_arr,(150,150))\n",
        "\n",
        "        x_test.append(img_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOo1M822uvj1"
      },
      "outputs": [],
      "source": [
        "train_x=np.array(x_train)\n",
        "test_x=np.array(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "te3bncE1uxgf"
      },
      "outputs": [],
      "source": [
        "train_x=train_x/255.0\n",
        "test_x=test_x/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTSLDm_4u0OB"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a0l7khiu5xQ",
        "outputId": "10b58e44-9be9-4321-bbcb-3b58cabc6840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3693 images belonging to 6 classes.\n",
            "Found 1058 images belonging to 6 classes.\n"
          ]
        }
      ],
      "source": [
        "training_set = train_datagen.flow_from_directory(train_path,\n",
        "                                                 target_size = (150, 150),\n",
        "                                                 batch_size = 10,\n",
        "                                                 class_mode = 'sparse')\n",
        "test_set = test_datagen.flow_from_directory(test_path,\n",
        "                                            target_size = (150, 150),\n",
        "                                            batch_size = 10,\n",
        "                                            class_mode = 'sparse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keJAKel9u_yW"
      },
      "outputs": [],
      "source": [
        "train_y=training_set.classes\n",
        "test_y=test_set.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjGotJ87vFow",
        "outputId": "4715e538-4bdb-435c-c837-1d56aadd5174"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Acne and Rosacea Photos': 0,\n",
              " 'Atopic Dermatitis Photos': 1,\n",
              " 'Herpes HPV and other STDs Photos': 2,\n",
              " 'Poison Ivy Photos and other Contact Dermatitis': 3,\n",
              " 'Psoriasis pictures Lichen Planus and related diseases': 4,\n",
              " 'Scabies Lyme Disease and other Infestations and Bites': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "training_set.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG6jCkenvHrv",
        "outputId": "590f310b-2776-42c7-a801-6777cddcede6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3693,), (1058,))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "train_y.shape,test_y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGlieogBfYUm"
      },
      "source": [
        "# MODELLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmdtxti-viA6"
      },
      "outputs": [],
      "source": [
        "# JALANIN\n",
        "def create_model():\n",
        "\n",
        "  ### START CODE HERE       \n",
        "\n",
        "  # Define the model\n",
        "  model = tf.keras.models.Sequential([ \n",
        "    tf.keras.layers.Conv2D(128,(3,3), activation = 'relu', input_shape=(150,150,3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128,(3,3), activation = 'relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "  ])\n",
        "  \n",
        "\n",
        "  model.compile(optimizer = RMSprop(learning_rate=0.001),\n",
        "                loss =  'sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  ### END CODE HERE       \n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPILING"
      ],
      "metadata": {
        "id": "zHKBX5i9OXkY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CB14zaQuvlOn"
      },
      "outputs": [],
      "source": [
        "model = create_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1NZUDoQvWT3"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  loss='sparse_categorical_crossentropy',\n",
        "  optimizer=\"adam\",\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hp0qC6j5v-DB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb50f68-b7f7-4111-81c7-f08798101619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "116/116 [==============================] - 21s 72ms/step - loss: 1.6664 - accuracy: 0.3393\n",
            "Epoch 2/50\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 1.6264 - accuracy: 0.3463\n",
            "Epoch 3/50\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 1.5915 - accuracy: 0.3566\n",
            "Epoch 4/50\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 1.5797 - accuracy: 0.3588\n",
            "Epoch 5/50\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 1.5516 - accuracy: 0.3688\n",
            "Epoch 6/50\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 1.5262 - accuracy: 0.3761\n",
            "Epoch 7/50\n",
            "116/116 [==============================] - 7s 64ms/step - loss: 1.5054 - accuracy: 0.3899\n",
            "Epoch 8/50\n",
            "116/116 [==============================] - 7s 65ms/step - loss: 1.4697 - accuracy: 0.3937\n",
            "Epoch 9/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 1.4322 - accuracy: 0.4235\n",
            "Epoch 10/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 1.3812 - accuracy: 0.4414\n",
            "Epoch 11/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 1.3425 - accuracy: 0.4636\n",
            "Epoch 12/50\n",
            "116/116 [==============================] - 8s 66ms/step - loss: 1.2740 - accuracy: 0.4831\n",
            "Epoch 13/50\n",
            "116/116 [==============================] - 8s 66ms/step - loss: 1.2173 - accuracy: 0.5213\n",
            "Epoch 14/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 1.1320 - accuracy: 0.5532\n",
            "Epoch 15/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 1.0650 - accuracy: 0.5757\n",
            "Epoch 16/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.9590 - accuracy: 0.6168\n",
            "Epoch 17/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.8959 - accuracy: 0.6585\n",
            "Epoch 18/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.8253 - accuracy: 0.6829\n",
            "Epoch 19/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.7520 - accuracy: 0.7149\n",
            "Epoch 20/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.6699 - accuracy: 0.7514\n",
            "Epoch 21/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.6571 - accuracy: 0.7617\n",
            "Epoch 22/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.5941 - accuracy: 0.7858\n",
            "Epoch 23/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.5312 - accuracy: 0.8061\n",
            "Epoch 24/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.5072 - accuracy: 0.8145\n",
            "Epoch 25/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.4653 - accuracy: 0.8262\n",
            "Epoch 26/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.4256 - accuracy: 0.8416\n",
            "Epoch 27/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.3940 - accuracy: 0.8570\n",
            "Epoch 28/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.3837 - accuracy: 0.8630\n",
            "Epoch 29/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.3594 - accuracy: 0.8738\n",
            "Epoch 30/50\n",
            "116/116 [==============================] - 8s 66ms/step - loss: 0.3594 - accuracy: 0.8803\n",
            "Epoch 31/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.3348 - accuracy: 0.8890\n",
            "Epoch 32/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.3159 - accuracy: 0.8925\n",
            "Epoch 33/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.2779 - accuracy: 0.9060\n",
            "Epoch 34/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.2824 - accuracy: 0.9060\n",
            "Epoch 35/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.2493 - accuracy: 0.9177\n",
            "Epoch 36/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.2228 - accuracy: 0.9239\n",
            "Epoch 37/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.2089 - accuracy: 0.9285\n",
            "Epoch 38/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.2058 - accuracy: 0.9277\n",
            "Epoch 39/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1998 - accuracy: 0.9323\n",
            "Epoch 40/50\n",
            "116/116 [==============================] - 8s 66ms/step - loss: 0.1976 - accuracy: 0.9320\n",
            "Epoch 41/50\n",
            "116/116 [==============================] - 8s 66ms/step - loss: 0.1732 - accuracy: 0.9385\n",
            "Epoch 42/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1759 - accuracy: 0.9439\n",
            "Epoch 43/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1912 - accuracy: 0.9402\n",
            "Epoch 44/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1715 - accuracy: 0.9415\n",
            "Epoch 45/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1592 - accuracy: 0.9453\n",
            "Epoch 46/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1643 - accuracy: 0.9494\n",
            "Epoch 47/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1389 - accuracy: 0.9542\n",
            "Epoch 48/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1401 - accuracy: 0.9529\n",
            "Epoch 49/50\n",
            "116/116 [==============================] - 8s 65ms/step - loss: 0.1189 - accuracy: 0.9605\n",
            "Epoch 50/50\n",
            "116/116 [==============================] - 8s 66ms/step - loss: 0.1213 - accuracy: 0.9618\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "  train_x,\n",
        "  train_y,\n",
        "  epochs=50,\n",
        "  batch_size=32,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0GHTwmdzWzH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
        "import numpy as np\n",
        "#predict\n",
        "y_pred=model.predict(test_x)\n",
        "y_pred=np.argmax(y_pred,axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6tHg5Kzzbe1",
        "outputId": "23283669-7392-4591-8169-07426e8ac5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.29      0.24       214\n",
            "           1       0.09      0.05      0.07       206\n",
            "           2       0.02      0.01      0.02       144\n",
            "           3       0.00      0.00      0.00        30\n",
            "           4       0.31      0.27      0.29       406\n",
            "           5       0.07      0.14      0.10        58\n",
            "\n",
            "    accuracy                           0.18      1058\n",
            "   macro avg       0.12      0.13      0.12      1058\n",
            "weighted avg       0.18      0.18      0.18      1058\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#get classification report\n",
        "print(classification_report(y_pred,test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM_oJvgwzdfs",
        "outputId": "767b26b4-877f-438c-b653-5acf530de9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 62  23  47   3  46  33]\n",
            " [ 13  11  51  54  73   4]\n",
            " [ 28   7   2   5  97   5]\n",
            " [ 10   2   1   0  15   2]\n",
            " [169  69   1   2 109  56]\n",
            " [ 26  11   0   1  12   8]]\n"
          ]
        }
      ],
      "source": [
        "#get confusion matrix\n",
        "print(confusion_matrix(y_pred,test_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing\n",
        "\n",
        "Referensi https://www.geeksforgeeks.org/python-image-classification-using-keras/"
      ],
      "metadata": {
        "id": "A_hSRWHvOipJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAkJ5xJJoHon",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "62536cbf-a571-4e74-ea62-361b637af5d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0bf6f539-4fc2-4dbd-be26-6c227a0d37e0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0bf6f539-4fc2-4dbd-be26-6c227a0d37e0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Lichen-Nitidus-3.jpg to Lichen-Nitidus-3.jpg\n",
            "[2.3164639e-01 5.1888373e-06 2.7756953e-06 5.1630114e-04 7.6782840e-01\n",
            " 8.7890504e-07]\n",
            "Psoriasis\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = x / 255\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if max(classes[0])==classes[0][0]:\n",
        "    print(\"Acne and Rosacea\")\n",
        "  elif max(classes[0])==classes[0][1]:\n",
        "    print(\"Atopic Dermatits\")\n",
        "  elif max(classes[0])==classes[0][2]:\n",
        "    print(\"Herpes HPV and other STD\")\n",
        "  elif max(classes[0])==classes[0][3]:\n",
        "    print(\"Poison Ivy and other contact diseases\")\n",
        "  elif max(classes[0])==classes[0][4]:\n",
        "    print(\"Psoriasis atau Lichen\")\n",
        "  elif max(classes[0])==classes[0][5]:\n",
        "    print(\"Scabies Lyme Disease and other infestations and bites\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrEHCj0ZI_EZ"
      },
      "source": [
        "# Save the Model\n",
        "\n",
        "To load the trained model into TensorFlow Serving we first need to save it in the [SavedModel](https://www.tensorflow.org/guide/saved_model) format.  This will create a protobuf file in a well-defined directory hierarchy, and will include a version number.  [TensorFlow Serving](https://www.tensorflow.org/tfx/serving/serving_config) allows us to select which version of a model, or \"servable\" we want to use when we make inference requests.  Each version will be exported to a different sub-directory under the given path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OIxd0LIDbQl"
      },
      "outputs": [],
      "source": [
        "# JALANIN\n",
        "# Save the trained model as a Keras HDF5 file. \n",
        "import time\n",
        "saved_model_path = \"./my_model.h5\"#.format(int(time.time()))\n",
        "\n",
        "model.save(saved_model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF LITE CONVERT"
      ],
      "metadata": {
        "id": "9Ckr24fB1Rz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tf_litemodel = converter.convert()\n",
        "\n",
        "with open('model.tflite','wb') as f:\n",
        "  f.write(tf_litemodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx58oT9v1d_b",
        "outputId": "a0c8a0c8-c0de-4124-874a-1f0e7df98d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmps98mhydh/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmps98mhydh/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6IyMnE_Tan6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eef5d03-ac6b-4341-838d-e817d06e0a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BucketNotFoundException: 404 gs://{bucket_name} bucket does not exist.\n"
          ]
        }
      ],
      "source": [
        "# Save model to GCP Cloud Bucket\n",
        "\n",
        "'''!gsutil -m cp -r /content/my_model.h5* gs://{bucket_name}/dataset'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQTEAyV3s5Qg"
      },
      "source": [
        "# Converting the model to TFJS\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOXS4OfWokfv"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow==2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I011wOlTr4ZM"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx3XGJkHrWtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9268557-db53-4066-afd2-9b13a183feb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "• Using TensorFlow Version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "#print('\\u2022 Using TensorFlow Version:', tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3p-hEqNnoYqY"
      },
      "outputs": [],
      "source": [
        "# Use the tensorflow.js converter to convert the saved Keras model into JSON format.\n",
        "\n",
        "!tensorflowjs_converter --input_format=keras {saved_model_path} ./"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BVaNuVpUesyT",
        "xVhnwKFhe19X",
        "vLlFzqOxep59",
        "fUZ7HXtEedAu",
        "6J-eobHrOnRz",
        "FoC60AHN96MQ"
      ],
      "name": "code.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "0e091db8d58018cc79bd95d4c344ae6fb50dc1dda4747c0206fad4c7e07cd6fa"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}